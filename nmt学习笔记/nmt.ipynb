{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2007 Google, Inc. All Rights Reserved.\n",
    "# Licensed to PSF under a Contributor Agreement.\n",
    "\n",
    "\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n",
    "\n",
    "from _weakrefset import WeakSet\n",
    "\n",
    "\n",
    "def abstractmethod(funcobj):\n",
    "    \"\"\"A decorator indicating abstract methods.\n",
    "\n",
    "    Requires that the metaclass is ABCMeta or derived from it.  A\n",
    "    class that has a metaclass derived from ABCMeta cannot be\n",
    "    instantiated unless all of its abstract methods are overridden.\n",
    "    The abstract methods can be called using any of the normal\n",
    "    'super' call mechanisms.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractmethod\n",
    "            def my_abstract_method(self, ...):\n",
    "                ...\n",
    "    \"\"\"\n",
    "    funcobj.__isabstractmethod__ = True\n",
    "    return funcobj\n",
    "\n",
    "\n",
    "class abstractclassmethod(classmethod):\n",
    "    \"\"\"\n",
    "    A decorator indicating abstract classmethods.\n",
    "\n",
    "    Similar to abstractmethod.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractclassmethod\n",
    "            def my_abstract_classmethod(cls, ...):\n",
    "                ...\n",
    "\n",
    "    'abstractclassmethod' is deprecated. Use 'classmethod' with\n",
    "    'abstractmethod' instead.\n",
    "    \"\"\"\n",
    "\n",
    "    __isabstractmethod__ = True\n",
    "\n",
    "    def __init__(self, callable):\n",
    "        callable.__isabstractmethod__ = True\n",
    "        super().__init__(callable)\n",
    "\n",
    "\n",
    "class abstractstaticmethod(staticmethod):\n",
    "    \"\"\"\n",
    "    A decorator indicating abstract staticmethods.\n",
    "\n",
    "    Similar to abstractmethod.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractstaticmethod\n",
    "            def my_abstract_staticmethod(...):\n",
    "                ...\n",
    "\n",
    "    'abstractstaticmethod' is deprecated. Use 'staticmethod' with\n",
    "    'abstractmethod' instead.\n",
    "    \"\"\"\n",
    "\n",
    "    __isabstractmethod__ = True\n",
    "\n",
    "    def __init__(self, callable):\n",
    "        callable.__isabstractmethod__ = True\n",
    "        super().__init__(callable)\n",
    "\n",
    "\n",
    "class abstractproperty(property):\n",
    "    \"\"\"\n",
    "    A decorator indicating abstract properties.\n",
    "\n",
    "    Requires that the metaclass is ABCMeta or derived from it.  A\n",
    "    class that has a metaclass derived from ABCMeta cannot be\n",
    "    instantiated unless all of its abstract properties are overridden.\n",
    "    The abstract properties can be called using any of the normal\n",
    "    'super' call mechanisms.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractproperty\n",
    "            def my_abstract_property(self):\n",
    "                ...\n",
    "\n",
    "    This defines a read-only property; you can also define a read-write\n",
    "    abstract property using the 'long' form of property declaration:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            def getx(self): ...\n",
    "            def setx(self, value): ...\n",
    "            x = abstractproperty(getx, setx)\n",
    "\n",
    "    'abstractproperty' is deprecated. Use 'property' with 'abstractmethod'\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    __isabstractmethod__ = True\n",
    "\n",
    "\n",
    "class ABCMeta(type):\n",
    "\n",
    "    \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n",
    "\n",
    "    Use this metaclass to create an ABC.  An ABC can be subclassed\n",
    "    directly, and then acts as a mix-in class.  You can also register\n",
    "    unrelated concrete classes (even built-in classes) and unrelated\n",
    "    ABCs as 'virtual subclasses' -- these and their descendants will\n",
    "    be considered subclasses of the registering ABC by the built-in\n",
    "    issubclass() function, but the registering ABC won't show up in\n",
    "    their MRO (Method Resolution Order) nor will method\n",
    "    implementations defined by the registering ABC be callable (not\n",
    "    even via super()).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # A global counter that is incremented each time a class is\n",
    "    # registered as a virtual subclass of anything.  It forces the\n",
    "    # negative cache to be cleared before its next use.\n",
    "    # Note: this counter is private. Use `abc.get_cache_token()` for\n",
    "    #       external code.\n",
    "    _abc_invalidation_counter = 0\n",
    "\n",
    "    def __new__(mcls, name, bases, namespace, **kwargs):\n",
    "        cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n",
    "        # Compute set of abstract method names\n",
    "        abstracts = {name\n",
    "                     for name, value in namespace.items()\n",
    "                     if getattr(value, \"__isabstractmethod__\", False)}\n",
    "        for base in bases:\n",
    "            for name in getattr(base, \"__abstractmethods__\", set()):\n",
    "                value = getattr(cls, name, None)\n",
    "                if getattr(value, \"__isabstractmethod__\", False):\n",
    "                    abstracts.add(name)\n",
    "        cls.__abstractmethods__ = frozenset(abstracts)\n",
    "        # Set up inheritance registry\n",
    "        cls._abc_registry = WeakSet()\n",
    "        cls._abc_cache = WeakSet()\n",
    "        cls._abc_negative_cache = WeakSet()\n",
    "        cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n",
    "        return cls\n",
    "\n",
    "    def register(cls, subclass):\n",
    "        \"\"\"Register a virtual subclass of an ABC.\n",
    "\n",
    "        Returns the subclass, to allow usage as a class decorator.\n",
    "        \"\"\"\n",
    "        if not isinstance(subclass, type):\n",
    "            raise TypeError(\"Can only register classes\")\n",
    "        if issubclass(subclass, cls):\n",
    "            return subclass  # Already a subclass\n",
    "        # Subtle: test for cycles *after* testing for \"already a subclass\";\n",
    "        # this means we allow X.register(X) and interpret it as a no-op.\n",
    "        if issubclass(cls, subclass):\n",
    "            # This would create a cycle, which is bad for the algorithm below\n",
    "            raise RuntimeError(\"Refusing to create an inheritance cycle\")\n",
    "        cls._abc_registry.add(subclass)\n",
    "        ABCMeta._abc_invalidation_counter += 1  # Invalidate negative cache\n",
    "        return subclass\n",
    "\n",
    "    def _dump_registry(cls, file=None):\n",
    "        \"\"\"Debug helper to print the ABC registry.\"\"\"\n",
    "        print(\"Class: %s.%s\" % (cls.__module__, cls.__qualname__), file=file)\n",
    "        print(\"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter, file=file)\n",
    "        for name in sorted(cls.__dict__.keys()):\n",
    "            if name.startswith(\"_abc_\"):\n",
    "                value = getattr(cls, name)\n",
    "                print(\"%s: %r\" % (name, value), file=file)\n",
    "\n",
    "    def __instancecheck__(cls, instance):\n",
    "        \"\"\"Override for isinstance(instance, cls).\"\"\"\n",
    "        # Inline the cache checking\n",
    "        subclass = instance.__class__\n",
    "        if subclass in cls._abc_cache:\n",
    "            return True\n",
    "        subtype = type(instance)\n",
    "        if subtype is subclass:\n",
    "            if (cls._abc_negative_cache_version ==\n",
    "                ABCMeta._abc_invalidation_counter and\n",
    "                subclass in cls._abc_negative_cache):\n",
    "                return False\n",
    "            # Fall back to the subclass check.\n",
    "            return cls.__subclasscheck__(subclass)\n",
    "        return any(cls.__subclasscheck__(c) for c in {subclass, subtype})\n",
    "\n",
    "    def __subclasscheck__(cls, subclass):\n",
    "        \"\"\"Override for issubclass(subclass, cls).\"\"\"\n",
    "        # Check cache\n",
    "        if subclass in cls._abc_cache:\n",
    "            return True\n",
    "        # Check negative cache; may have to invalidate\n",
    "        if cls._abc_negative_cache_version < ABCMeta._abc_invalidation_counter:\n",
    "            # Invalidate the negative cache\n",
    "            cls._abc_negative_cache = WeakSet()\n",
    "            cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n",
    "        elif subclass in cls._abc_negative_cache:\n",
    "            return False\n",
    "        # Check the subclass hook\n",
    "        ok = cls.__subclasshook__(subclass)\n",
    "        if ok is not NotImplemented:\n",
    "            assert isinstance(ok, bool)\n",
    "            if ok:\n",
    "                cls._abc_cache.add(subclass)\n",
    "            else:\n",
    "                cls._abc_negative_cache.add(subclass)\n",
    "            return ok\n",
    "        # Check if it's a direct subclass\n",
    "        if cls in getattr(subclass, '__mro__', ()):\n",
    "            cls._abc_cache.add(subclass)\n",
    "            return True\n",
    "        # Check if it's a subclass of a registered class (recursive)\n",
    "        for rcls in cls._abc_registry:\n",
    "            if issubclass(subclass, rcls):\n",
    "                cls._abc_cache.add(subclass)\n",
    "                return True\n",
    "        # Check if it's a subclass of a subclass (recursive)\n",
    "        for scls in cls.__subclasses__():\n",
    "            if issubclass(subclass, scls):\n",
    "                cls._abc_cache.add(subclass)\n",
    "                return True\n",
    "        # No dice; update negative cache\n",
    "        cls._abc_negative_cache.add(subclass)\n",
    "        return False\n",
    "\n",
    "\n",
    "class ABC(metaclass=ABCMeta):\n",
    "    \"\"\"Helper class that provides a standard way to create an ABC using\n",
    "    inheritance.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_cache_token():\n",
    "    \"\"\"Returns the current ABC cache token.\n",
    "\n",
    "    The token is an opaque object (supporting equality testing) identifying the\n",
    "    current version of the ABC cache for virtual subclasses. The token changes\n",
    "    with every call to ``register()`` on any ABC.\n",
    "    \"\"\"\n",
    "    return ABCMeta._abc_invalidation_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initializer(init_op, seed=None, init_weight=None):\n",
    "    \"\"\"Create an initializer. init_weight is only for uniform.\"\"\"\n",
    "    if init_op == \"uniform\":\n",
    "        assert init_weight\n",
    "        return tf.random_uniform_initializer(-init_weight, init_weight, seed=seed)\n",
    "    elif init_op == \"glorot_normal\":#以0为中心的截断正态分布中抽取样本\n",
    "        return tf.keras.initializers.glorot_normal(seed=seed)\n",
    "    elif init_op == \"glorot_uniform\":#均匀分布初始化\n",
    "        return tf.keras.initializers.glorot_uniform(seed=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown init_op %s\" % init_op)\n",
    "def get_device_str(device_id, num_gpus):\n",
    "    \"\"\"Return a device string for multi-GPU setup.\"\"\"\n",
    "    if num_gpus == 0:\n",
    "        return \"/cpu:0\"\n",
    "    device_str_output = \"/gpu:%d\" % (device_id % num_gpus)\n",
    "    return device_str_output\n",
    "def _single_cell(unit_type, num_units, forget_bias, dropout, mode,\n",
    "                 residual_connection=False, device_str=None, residual_fn=None):\n",
    "    dropout = dropout if mode == tf.contrib.learn.ModeKeys.TRAIN else 0.0\n",
    "    # Cell Type\n",
    "    if unit_type == \"lstm\":\n",
    "        single_cell=tf.contrib.rnn.BasicLSTMCell(num_units=num_units,forget_bias=forget_bias)\n",
    "    elif unit_type==\"gru\":\n",
    "        single_cell = tf.contrib.rnn.GRUCell(num_units)\n",
    "    elif unit_type == \"layer_norm_lstm\":\n",
    "        #LSTM unit with layer normalization and recurrent dropout.\n",
    "        single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units,forget_bias=forget_bias,layer_norm=True)\n",
    "    elif unit_type == \"nas\":\n",
    "        #Neural Architecture Search (NAS) recurrent network cell.\n",
    "        #RNN作为一个 controller去生成模型的描述符，然后根据描述符得到模型，\n",
    "        #进而得到该模型在数据集上的准确度。接着将该准确度作为奖励信号(reward signal)对controller进行更新。\n",
    "        #如此不断迭代找到合适的网络结构。         \n",
    "        single_cell = tf.contrib.rnn.NASCell(num_units)\n",
    "    else：\n",
    "        raise ValueError(\"Unknown unit type %s!\" % unit_type)\n",
    "    # Dropout (= 1 - keep_prob)\n",
    "    if dropout > 0.0:\n",
    "        single_cell = tf.contrib.rnn.DropoutWrapper(cell=single_cell, input_keep_prob=(1.0 - dropout))\n",
    "    # Residual\n",
    "    if residual_connection:\n",
    "        single_cell=tf.contrib.rnn.ResidualWrapper(single_cell,residual_fn=residual_fn)\n",
    "    # Device Wrapper\n",
    "    if device_str:\n",
    "        single_cell = tf.contrib.rnn.DeviceWrapper(single_cell, device_str)\n",
    "    return single_cell\n",
    "    \n",
    "\n",
    "def create_rnn_cell(unit_type, num_units, num_layers, num_residual_layers,\n",
    "                    forget_bias, dropout, mode, num_gpus, base_gpu=0,single_cell_fn=None):\n",
    "    if not single_cell_fn:\n",
    "        single_cell_fn = _single_cell\n",
    "    cell_list = []\n",
    "    cell_list = _cell_list(unit_type=unit_type,\n",
    "                         num_units=num_units,\n",
    "                         num_layers=num_layers,\n",
    "                         num_residual_layers=num_residual_layers,\n",
    "                         forget_bias=forget_bias,\n",
    "                         dropout=dropout,\n",
    "                         mode=mode,\n",
    "                         num_gpus=num_gpus,\n",
    "                         base_gpu=base_gpu,\n",
    "                         single_cell_fn=single_cell_fn)\n",
    "    \n",
    "    if len(cell_list)==1:\n",
    "        return cell_list[0]\n",
    "    else:\n",
    "        return tf.contrib.rnn.MultiRNNCell(cell_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core\n",
    "# 基础的seq2seq model\n",
    "class BaseModel(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams,#超参数\n",
    "        mode,#train/eval/infer/\n",
    "        iterator,#迭代次数\n",
    "        source_vocab_table,#\n",
    "        target_vocab_table,#\n",
    "        reverse_target_vocab_table=None,#Lookup table mapping ids to target words. Only required in INFER mode. Defaults to None.\n",
    "        scope=None,#模型的scope\n",
    "        extra_args=None):#model_helper.ExtraArgs, for passing customizable functions.\n",
    "    self.iterator = iterator\n",
    "    self.mode = mode\n",
    "    self.src_vocab_table = source_vocab_table\n",
    "    self.tgt_vocab_table = target_vocab_table\n",
    "\n",
    "    self.src_vocab_size = hparams.src_vocab_size\n",
    "    self.tgt_vocab_size = hparams.tgt_vocab_size\n",
    "    self.num_gpus = hparams.num_gpus\n",
    "    self.time_major = hparams.time_major\n",
    "\n",
    "    \n",
    "    self.single_cell_fn = None\n",
    "    # extra_args: to make it flexible for adding external customizable code\n",
    "    if extra_args:\n",
    "        self.single_cell_fn = extra_args.single_cell_fn\n",
    "    # Set num layers\n",
    "    self.num_encoder_layers = hparams.num_encoder_layers\n",
    "    self.num_decoder_layers = hparams.num_decoder_layers\n",
    "    # Set num residual layers\n",
    "    if hasattr(hparams, \"num_residual_layers\"):  # compatible common_test_utils\n",
    "        self.num_encoder_residual_layers = hparams.num_residual_layers\n",
    "        self.num_decoder_residual_layers = hparams.num_residual_layers\n",
    "    else:\n",
    "        self.num_encoder_residual_layers = hparams.num_encoder_residual_layers\n",
    "        self.num_decoder_residual_layers = hparams.num_decoder_residual_layers\n",
    "        \n",
    "    # Initializer\n",
    "    initializer=get_initializer(hparams.init_op, hparams.random_seed, hparams.init_weight)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    # Projection\n",
    "    with tf.variable_scope(scope or 'build_network'):\n",
    "        with tf.variable_scope(\"decoder/output_projection\"):\n",
    "            self.output_layer = core.Dense(hparams.tgt_vocab_size, use_bias=False, name=\"output_projection\")\n",
    "            \n",
    "    ## Train graph\n",
    "    res = self.build_graph(hparams, scope=scope)\n",
    "    \n",
    "    if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "        self.train_loss = res[1]\n",
    "        self.word_count=tf.reduce_sum(self.iterator.source_sequence_length)+tf.reduce_sum(self.iterator.target_sequence_length)\n",
    "    elif self.mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "        self.eval_loss = res[1]\n",
    "    elif self.mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "        self.infer_logits, _, self.final_context_state, self.sample_id = res\n",
    "        self.sample_words = reverse_target_vocab_table.lookup(tf.to_int64(self.sample_id))\n",
    "    \n",
    "    if self.mode != tf.contrib.learn.ModeKeys.INFER:\n",
    "        self.predict_count = tf.reduce_sum(self.iterator.target_sequence_length)\n",
    "    \n",
    "    self.global_step= tf.Variable(0,trainable=False)\n",
    "    params = tf.trainable_variables()\n",
    "        \n",
    "    if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "        self.learning_rate = tf.constant(hparams.learning_rate)\n",
    "        # warm-up\n",
    "        self.learning_rate = self._get_learning_rate_warmup(hparams)\n",
    "        # decay\n",
    "        self.learning_rate = self._get_learning_rate_decay(hparams)\n",
    "        \n",
    "        \n",
    "    \"\"\"Subclass must implement this method.\"\"\"\n",
    "    def build_graph(self, hparams, scope=None):\n",
    "        with tf.variable_scope(scope or \"dynamic_seq2seq\", dtype=dtype):\n",
    "            # Encoder\n",
    "            encoder_outputs, encoder_state = self._build_encoder(hparams)\n",
    "            ## Decoder\n",
    "            logits, sample_id, final_context_state = self._build_decoder(encoder_outputs, encoder_state, hparams)\n",
    "            ## Loss\n",
    "            if self.mode != tf.contrib.learn.ModeKeys.INFER:\n",
    "                with tf.device(get_device_str(self.num_encoder_layers - 1,self.num_gpus)):\n",
    "                loss = self._compute_loss(logits)\n",
    "            else:\n",
    "                loss = None\n",
    "            return logits, loss, final_context_state, sample_id\n",
    "    \n",
    "    def _compute_loss(self,logits):\n",
    "        target_output = self.iterator.target_output\n",
    "        if self.time_major:\n",
    "            target_output = tf.transpose(target_output)\n",
    "        max_time = self.get_max_time(target_output)\n",
    "        #sparse_softmax_cross_entropy_with_logits:lables接受直接的数字标签\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_output,logits=logits)\n",
    "        '''\n",
    "        tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
    "                                  #  [True, True, True, False, False],\n",
    "                                  #  [True, True, False, False, False]]\n",
    "\n",
    "        tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],\n",
    "                                    #   [True, True, True]],\n",
    "                                    #  [[True, True, False],\n",
    "                                    #   [False, False, False]]]\n",
    "        '''\n",
    "        target_weights = tf.sequence_mask(self.iterator.target_sequence_length,max_time,dtype=logits.dtype)\n",
    "        if self.time_major:\n",
    "            target_weights=tf.transpose(target_weights)\n",
    "        \n",
    "        loss = tf.reduce_sum(crossent*target_weights)/tf.to_float(self.batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def _get_learning_rate_warmup(self,hparams):\n",
    "        \"\"\"Get learning rate warmup.调整学习率退火方案和动量参数的SGD不仅可以与Adam竞争，而且收敛速度更快。\"\"\"\n",
    "        warmup_steps = hparams.warmup_steps\n",
    "        warmup_scheme = hparams.warmup_scheme\n",
    "        # When step < warmup_steps,\n",
    "        #   learing_rate *= warmup_factor ** (warmup_steps - step)\n",
    "        if warmup_scheme == \"t2t\":\n",
    "            # 0.01^(1/warmup_steps): we start with a lr, 100 times smaller\n",
    "            warmup_factor = tf.exp(tf.log(0.01) / warmup_steps)\n",
    "            inv_decay = warmup_factor**(tf.to_float(warmup_steps - self.global_step))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown warmup scheme %s\" % warmup_scheme)\n",
    "        return tf.cond(self.global_step < hparams.warmup_steps,\n",
    "                       lambda: inv_decay * self.learning_rate,\n",
    "                       lambda: self.learning_rate,\n",
    "                       name=\"learning_rate_warump_cond\")\n",
    "    \n",
    "    def _get_learning_rate_decay(self, hparams):\n",
    "        \"\"\"Get learning rate decay.\"\"\"\n",
    "        if hparams.decay_scheme in [\"luong5\", \"luong10\", \"luong234\"]:\n",
    "            decay_factor = 0.5\n",
    "            if hparams.decay_scheme == \"luong5\":\n",
    "                start_decay_step = int(hparams.num_train_steps / 2)\n",
    "                decay_times = 5\n",
    "            elif hparams.decay_scheme == \"luong10\":\n",
    "                start_decay_step = int(hparams.num_train_steps / 2)\n",
    "                decay_times = 10\n",
    "            elif hparams.decay_scheme == \"luong234\":\n",
    "                start_decay_step = int(hparams.num_train_steps * 2 / 3)\n",
    "                decay_times = 4\n",
    "            remain_steps = hparams.num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / decay_times)\n",
    "        elif not hparams.decay_scheme:  # no decay\n",
    "            start_decay_step = hparams.num_train_steps\n",
    "            decay_steps = 0\n",
    "            decay_factor = 1.0\n",
    "        elif hparams.decay_scheme:\n",
    "            raise ValueError(\"Unknown decay scheme %s\" % hparams.decay_scheme)\n",
    "        return tf.cond(self.global_step < start_decay_step,\n",
    "                        lambda: self.learning_rate,\n",
    "                        lambda: tf.train.exponential_decay(\n",
    "                        self.learning_rate,\n",
    "                        (self.global_step - start_decay_step),\n",
    "                        decay_steps, decay_factor, staircase=True),\n",
    "                        name=\"learning_rate_decay_cond\")\n",
    "    def init_embeddings(self, hparams, scope):\n",
    "        \"\"\"Init embeddings.\"\"\"\n",
    "        self.embedding_encoder, self.embedding_decoder = (\n",
    "            model_helper.create_emb_for_encoder_and_decoder(\n",
    "                share_vocab=hparams.share_vocab,\n",
    "                src_vocab_size=self.src_vocab_size,\n",
    "                tgt_vocab_size=self.tgt_vocab_size,\n",
    "                src_embed_size=hparams.num_units,\n",
    "                tgt_embed_size=hparams.num_units,\n",
    "                num_partitions=hparams.num_embeddings_partitions,\n",
    "                src_vocab_file=hparams.src_vocab_file,\n",
    "                tgt_vocab_file=hparams.tgt_vocab_file,\n",
    "                src_embed_file=hparams.src_embed_file,\n",
    "                tgt_embed_file=hparams.tgt_embed_file,\n",
    "                scope=scope,))\n",
    "    def train(self, sess):\n",
    "        assert self.mode == tf.contrib.learn.ModeKeys.TRAIN\n",
    "        return sess.run([self.update,\n",
    "                         self.train_loss,\n",
    "                         self.predict_count,\n",
    "                         self.train_summary,\n",
    "                         self.global_step,\n",
    "                         self.word_count,\n",
    "                         self.batch_size,\n",
    "                         self.grad_norm,\n",
    "                         self.learning_rate])\n",
    "    def eval(self, sess):\n",
    "        assert self.mode == tf.contrib.learn.ModeKeys.EVAL\n",
    "        return sess.run([self.eval_loss,\n",
    "                         self.predict_count,\n",
    "                         self.batch_size])\n",
    "    @abstractmethod\n",
    "    def _build_encoder(self,hparams):\n",
    "        pass\n",
    "    def _build_encoder_cell(self, hparams, num_layers, num_residual_layers,base_gpu=0):\n",
    "        return create_rnn_cell(\n",
    "            unit_type=hparams.unit_type,\n",
    "            num_units=hparams.num_units,\n",
    "            num_layers=num_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            forget_bias=hparams.forget_bias,\n",
    "            dropout=hparams.dropout,\n",
    "            num_gpus=hparams.num_gpus,\n",
    "            mode=self.mode,\n",
    "            base_gpu=base_gpu,\n",
    "            single_cell_fn=self.single_cell_fn)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态rnn注意力seq2seq\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
