{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2007 Google, Inc. All Rights Reserved.\n",
    "# Licensed to PSF under a Contributor Agreement.\n",
    "\n",
    "\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n",
    "\n",
    "from _weakrefset import WeakSet\n",
    "\n",
    "\n",
    "def abstractmethod(funcobj):\n",
    "    \"\"\"A decorator indicating abstract methods.\n",
    "\n",
    "    Requires that the metaclass is ABCMeta or derived from it.  A\n",
    "    class that has a metaclass derived from ABCMeta cannot be\n",
    "    instantiated unless all of its abstract methods are overridden.\n",
    "    The abstract methods can be called using any of the normal\n",
    "    'super' call mechanisms.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractmethod\n",
    "            def my_abstract_method(self, ...):\n",
    "                ...\n",
    "    \"\"\"\n",
    "    funcobj.__isabstractmethod__ = True\n",
    "    return funcobj\n",
    "\n",
    "\n",
    "class abstractclassmethod(classmethod):\n",
    "    \"\"\"\n",
    "    A decorator indicating abstract classmethods.\n",
    "\n",
    "    Similar to abstractmethod.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractclassmethod\n",
    "            def my_abstract_classmethod(cls, ...):\n",
    "                ...\n",
    "\n",
    "    'abstractclassmethod' is deprecated. Use 'classmethod' with\n",
    "    'abstractmethod' instead.\n",
    "    \"\"\"\n",
    "\n",
    "    __isabstractmethod__ = True\n",
    "\n",
    "    def __init__(self, callable):\n",
    "        callable.__isabstractmethod__ = True\n",
    "        super().__init__(callable)\n",
    "\n",
    "\n",
    "class abstractstaticmethod(staticmethod):\n",
    "    \"\"\"\n",
    "    A decorator indicating abstract staticmethods.\n",
    "\n",
    "    Similar to abstractmethod.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractstaticmethod\n",
    "            def my_abstract_staticmethod(...):\n",
    "                ...\n",
    "\n",
    "    'abstractstaticmethod' is deprecated. Use 'staticmethod' with\n",
    "    'abstractmethod' instead.\n",
    "    \"\"\"\n",
    "\n",
    "    __isabstractmethod__ = True\n",
    "\n",
    "    def __init__(self, callable):\n",
    "        callable.__isabstractmethod__ = True\n",
    "        super().__init__(callable)\n",
    "\n",
    "\n",
    "class abstractproperty(property):\n",
    "    \"\"\"\n",
    "    A decorator indicating abstract properties.\n",
    "\n",
    "    Requires that the metaclass is ABCMeta or derived from it.  A\n",
    "    class that has a metaclass derived from ABCMeta cannot be\n",
    "    instantiated unless all of its abstract properties are overridden.\n",
    "    The abstract properties can be called using any of the normal\n",
    "    'super' call mechanisms.\n",
    "\n",
    "    Usage:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            @abstractproperty\n",
    "            def my_abstract_property(self):\n",
    "                ...\n",
    "\n",
    "    This defines a read-only property; you can also define a read-write\n",
    "    abstract property using the 'long' form of property declaration:\n",
    "\n",
    "        class C(metaclass=ABCMeta):\n",
    "            def getx(self): ...\n",
    "            def setx(self, value): ...\n",
    "            x = abstractproperty(getx, setx)\n",
    "\n",
    "    'abstractproperty' is deprecated. Use 'property' with 'abstractmethod'\n",
    "    instead.\n",
    "    \"\"\"\n",
    "\n",
    "    __isabstractmethod__ = True\n",
    "\n",
    "\n",
    "class ABCMeta(type):\n",
    "\n",
    "    \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n",
    "\n",
    "    Use this metaclass to create an ABC.  An ABC can be subclassed\n",
    "    directly, and then acts as a mix-in class.  You can also register\n",
    "    unrelated concrete classes (even built-in classes) and unrelated\n",
    "    ABCs as 'virtual subclasses' -- these and their descendants will\n",
    "    be considered subclasses of the registering ABC by the built-in\n",
    "    issubclass() function, but the registering ABC won't show up in\n",
    "    their MRO (Method Resolution Order) nor will method\n",
    "    implementations defined by the registering ABC be callable (not\n",
    "    even via super()).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # A global counter that is incremented each time a class is\n",
    "    # registered as a virtual subclass of anything.  It forces the\n",
    "    # negative cache to be cleared before its next use.\n",
    "    # Note: this counter is private. Use `abc.get_cache_token()` for\n",
    "    #       external code.\n",
    "    _abc_invalidation_counter = 0\n",
    "\n",
    "    def __new__(mcls, name, bases, namespace, **kwargs):\n",
    "        cls = super().__new__(mcls, name, bases, namespace, **kwargs)\n",
    "        # Compute set of abstract method names\n",
    "        abstracts = {name\n",
    "                     for name, value in namespace.items()\n",
    "                     if getattr(value, \"__isabstractmethod__\", False)}\n",
    "        for base in bases:\n",
    "            for name in getattr(base, \"__abstractmethods__\", set()):\n",
    "                value = getattr(cls, name, None)\n",
    "                if getattr(value, \"__isabstractmethod__\", False):\n",
    "                    abstracts.add(name)\n",
    "        cls.__abstractmethods__ = frozenset(abstracts)\n",
    "        # Set up inheritance registry\n",
    "        cls._abc_registry = WeakSet()\n",
    "        cls._abc_cache = WeakSet()\n",
    "        cls._abc_negative_cache = WeakSet()\n",
    "        cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n",
    "        return cls\n",
    "\n",
    "    def register(cls, subclass):\n",
    "        \"\"\"Register a virtual subclass of an ABC.\n",
    "\n",
    "        Returns the subclass, to allow usage as a class decorator.\n",
    "        \"\"\"\n",
    "        if not isinstance(subclass, type):\n",
    "            raise TypeError(\"Can only register classes\")\n",
    "        if issubclass(subclass, cls):\n",
    "            return subclass  # Already a subclass\n",
    "        # Subtle: test for cycles *after* testing for \"already a subclass\";\n",
    "        # this means we allow X.register(X) and interpret it as a no-op.\n",
    "        if issubclass(cls, subclass):\n",
    "            # This would create a cycle, which is bad for the algorithm below\n",
    "            raise RuntimeError(\"Refusing to create an inheritance cycle\")\n",
    "        cls._abc_registry.add(subclass)\n",
    "        ABCMeta._abc_invalidation_counter += 1  # Invalidate negative cache\n",
    "        return subclass\n",
    "\n",
    "    def _dump_registry(cls, file=None):\n",
    "        \"\"\"Debug helper to print the ABC registry.\"\"\"\n",
    "        print(\"Class: %s.%s\" % (cls.__module__, cls.__qualname__), file=file)\n",
    "        print(\"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter, file=file)\n",
    "        for name in sorted(cls.__dict__.keys()):\n",
    "            if name.startswith(\"_abc_\"):\n",
    "                value = getattr(cls, name)\n",
    "                print(\"%s: %r\" % (name, value), file=file)\n",
    "\n",
    "    def __instancecheck__(cls, instance):\n",
    "        \"\"\"Override for isinstance(instance, cls).\"\"\"\n",
    "        # Inline the cache checking\n",
    "        subclass = instance.__class__\n",
    "        if subclass in cls._abc_cache:\n",
    "            return True\n",
    "        subtype = type(instance)\n",
    "        if subtype is subclass:\n",
    "            if (cls._abc_negative_cache_version ==\n",
    "                ABCMeta._abc_invalidation_counter and\n",
    "                subclass in cls._abc_negative_cache):\n",
    "                return False\n",
    "            # Fall back to the subclass check.\n",
    "            return cls.__subclasscheck__(subclass)\n",
    "        return any(cls.__subclasscheck__(c) for c in {subclass, subtype})\n",
    "\n",
    "    def __subclasscheck__(cls, subclass):\n",
    "        \"\"\"Override for issubclass(subclass, cls).\"\"\"\n",
    "        # Check cache\n",
    "        if subclass in cls._abc_cache:\n",
    "            return True\n",
    "        # Check negative cache; may have to invalidate\n",
    "        if cls._abc_negative_cache_version < ABCMeta._abc_invalidation_counter:\n",
    "            # Invalidate the negative cache\n",
    "            cls._abc_negative_cache = WeakSet()\n",
    "            cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n",
    "        elif subclass in cls._abc_negative_cache:\n",
    "            return False\n",
    "        # Check the subclass hook\n",
    "        ok = cls.__subclasshook__(subclass)\n",
    "        if ok is not NotImplemented:\n",
    "            assert isinstance(ok, bool)\n",
    "            if ok:\n",
    "                cls._abc_cache.add(subclass)\n",
    "            else:\n",
    "                cls._abc_negative_cache.add(subclass)\n",
    "            return ok\n",
    "        # Check if it's a direct subclass\n",
    "        if cls in getattr(subclass, '__mro__', ()):\n",
    "            cls._abc_cache.add(subclass)\n",
    "            return True\n",
    "        # Check if it's a subclass of a registered class (recursive)\n",
    "        for rcls in cls._abc_registry:\n",
    "            if issubclass(subclass, rcls):\n",
    "                cls._abc_cache.add(subclass)\n",
    "                return True\n",
    "        # Check if it's a subclass of a subclass (recursive)\n",
    "        for scls in cls.__subclasses__():\n",
    "            if issubclass(subclass, scls):\n",
    "                cls._abc_cache.add(subclass)\n",
    "                return True\n",
    "        # No dice; update negative cache\n",
    "        cls._abc_negative_cache.add(subclass)\n",
    "        return False\n",
    "\n",
    "\n",
    "class ABC(metaclass=ABCMeta):\n",
    "    \"\"\"Helper class that provides a standard way to create an ABC using\n",
    "    inheritance.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_cache_token():\n",
    "    \"\"\"Returns the current ABC cache token.\n",
    "\n",
    "    The token is an opaque object (supporting equality testing) identifying the\n",
    "    current version of the ABC cache for virtual subclasses. The token changes\n",
    "    with every call to ``register()`` on any ABC.\n",
    "    \"\"\"\n",
    "    return ABCMeta._abc_invalidation_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initializer(init_op, seed=None, init_weight=None):\n",
    "    \"\"\"Create an initializer. init_weight is only for uniform.\"\"\"\n",
    "    if init_op == \"uniform\":\n",
    "        assert init_weight\n",
    "        return tf.random_uniform_initializer(-init_weight, init_weight, seed=seed)\n",
    "    elif init_op == \"glorot_normal\":#以0为中心的截断正态分布中抽取样本\n",
    "        return tf.keras.initializers.glorot_normal(seed=seed)\n",
    "    elif init_op == \"glorot_uniform\":#均匀分布初始化\n",
    "        return tf.keras.initializers.glorot_uniform(seed=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown init_op %s\" % init_op)\n",
    "def get_device_str(device_id, num_gpus):\n",
    "    \"\"\"Return a device string for multi-GPU setup.\"\"\"\n",
    "    if num_gpus == 0:\n",
    "        return \"/cpu:0\"\n",
    "    device_str_output = \"/gpu:%d\" % (device_id % num_gpus)\n",
    "    return device_str_output\n",
    "def _single_cell(unit_type, num_units, forget_bias, dropout, mode,\n",
    "                 residual_connection=False, device_str=None, residual_fn=None):\n",
    "    dropout = dropout if mode == tf.contrib.learn.ModeKeys.TRAIN else 0.0\n",
    "    # Cell Type\n",
    "    if unit_type == \"lstm\":\n",
    "        single_cell=tf.contrib.rnn.BasicLSTMCell(num_units=num_units,forget_bias=forget_bias)\n",
    "    elif unit_type==\"gru\":\n",
    "        single_cell = tf.contrib.rnn.GRUCell(num_units)\n",
    "    elif unit_type == \"layer_norm_lstm\":\n",
    "        #LSTM unit with layer normalization and recurrent dropout.\n",
    "        single_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units,forget_bias=forget_bias,layer_norm=True)\n",
    "    elif unit_type == \"nas\":\n",
    "        #Neural Architecture Search (NAS) recurrent network cell.\n",
    "        #RNN作为一个 controller去生成模型的描述符，然后根据描述符得到模型，\n",
    "        #进而得到该模型在数据集上的准确度。接着将该准确度作为奖励信号(reward signal)对controller进行更新。\n",
    "        #如此不断迭代找到合适的网络结构。         \n",
    "        single_cell = tf.contrib.rnn.NASCell(num_units)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown unit type %s!\" % unit_type)\n",
    "    # Dropout (= 1 - keep_prob)\n",
    "    if dropout > 0.0:\n",
    "        single_cell = tf.contrib.rnn.DropoutWrapper(cell=single_cell, input_keep_prob=(1.0 - dropout))\n",
    "    # Residual\n",
    "    # RNNCell wrapper that ensures cell inputs are added to the outputs.     \n",
    "    if residual_connection:\n",
    "        single_cell=tf.contrib.rnn.ResidualWrapper(single_cell,residual_fn=residual_fn)\n",
    "    # Device Wrapper\n",
    "    if device_str:\n",
    "        single_cell = tf.contrib.rnn.DeviceWrapper(single_cell, device_str)\n",
    "    return single_cell\n",
    "    \n",
    "\n",
    "def create_rnn_cell(unit_type, num_units, num_layers, num_residual_layers,\n",
    "                    forget_bias, dropout, mode, num_gpus, base_gpu=0,single_cell_fn=None):\n",
    "    cell_list = []\n",
    "    cell_list = _cell_list(unit_type=unit_type,\n",
    "                         num_units=num_units,\n",
    "                         num_layers=num_layers,\n",
    "                         num_residual_layers=num_residual_layers,\n",
    "                         forget_bias=forget_bias,\n",
    "                         dropout=dropout,\n",
    "                         mode=mode,\n",
    "                         num_gpus=num_gpus,\n",
    "                         base_gpu=base_gpu,\n",
    "                         single_cell_fn=single_cell_fn)\n",
    "    \n",
    "    if len(cell_list)==1:\n",
    "        return cell_list[0]\n",
    "    else:\n",
    "        return tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "\n",
    "def _cell_list(unit_type, num_units, num_layers, num_residual_layers,\n",
    "               forget_bias, dropout, mode, num_gpus, base_gpu=0,\n",
    "               single_cell_fn=None, residual_fn=None):\n",
    "    \"\"\"Create a list of RNN cells.\"\"\"\n",
    "    if not single_cell_fn:\n",
    "        single_cell_fn = _single_cell\n",
    "    \n",
    "    #Multi-GPU\n",
    "    cell_list=[]\n",
    "    for i in range(num_layers):\n",
    "        single_cell = single_cell_fn(\n",
    "            unit_type=unit_type,\n",
    "            num_units=num_units,\n",
    "            forget_bias=forget_bias,\n",
    "            dropout=dropout,\n",
    "            mode=mode,\n",
    "            residual_connection=(i >= num_layers - num_residual_layers),\n",
    "            device_str=get_device_str(i + base_gpu, num_gpus),\n",
    "            residual_fn=residual_fn)\n",
    "        cell_list.append(single_cell)\n",
    "    return cell_list\n",
    "\n",
    "\n",
    "def _create_attention_images_summary(final_context_state):\n",
    "    \"\"\"create attention image and attention summary.\"\"\"\n",
    "    attention_images = (final_context_state.alignment_history.stack())\n",
    "    # Reshape to (batch, src_seq_len, tgt_seq_len,1)\n",
    "    attention_images = tf.expand_dims(tf.transpose(attention_images, [1, 2, 0]), -1)\n",
    "    # Scale to range [0, 255]\n",
    "    attention_images *= 255\n",
    "    attention_summary = tf.summary.image(\"attention_images\", attention_images)\n",
    "    return attention_summary   \n",
    "def create_attention_mechanism(attention_option, num_units, memory,source_sequence_length, mode):\n",
    "    \"\"\"Create attention mechanism based on the attention_option.\"\"\"\n",
    "    del mode  # unused\n",
    "    # Mechanism\n",
    "    if attention_option==\"luong\":\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units, memory, memory_sequence_length=source_sequence_length)\n",
    "    elif attention_option == \"scaled_luong\":\n",
    "        attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units,memory,memory_sequence_length=source_sequence_length,scale=True)\n",
    "    elif attention_option == \"bahdanau\":\n",
    "        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units,memory,memory_sequence_length=source_sequence_length,normalize=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown attention option %s\" % attention_option)\n",
    "    return attention_mechanism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Helper类是抽象基础类，其中定义了几个抽象方法，如initialize、sample、next_inputs，接下来所有的具体类都是继承自Helper抽象类。\n",
    "CustomHelper类虽然是继承自Helper类的一个具体类，但是这个类没有外加太多约束，它需要用户自定义initialize_fn, sample_fn, \n",
    "next_inputs_fn这三个函数，而InferenceHelper类我们可以看成是CustomHelper类的一个特殊情况，由于这个类只在推断的时候使用，\n",
    "因此在next_inputs函数中只需要将前一时刻的抽样结果作为下一时刻的输入即可；GreedyEmbeddingHelper类也是用于推理过程，\n",
    "不过它是采取argmax抽样算法来得到输出id，并且经过embedding层作为下一时刻的输入；\n",
    "而SampleEmbeddingHelper是继承自GreedyEmbeddingHelper类的一个类，与GreedyEmbeddingHelper类不同的是，\n",
    "SampleEmbeddingLayer是通过抽样算法来得到解码器的输出。\n",
    "TrainingHelper类也是继承自Helper类的一个具体类，在sample过程中，它采用的是最简单的argmax算法；\n",
    "而ScheduledEmbeddingTrainingHelper类是继承自TrainingHelper类，其中的sample算法采取的是广义伯努利算法，\n",
    "并且并不是每一个时刻都会采样，同时这里添加了embedding操作，即根据解码器的输出id从embedding矩阵中查找其对应的embedding向量；\n",
    "ScheduledOutputTrainingHelper类同样也是继承自TrainingHelper类，没有embedding操作，直接对输出进行抽样。\n",
    "TrainingHelper：适用于训练的helper。\n",
    "InferenceHelper：适用于测试的helper。\n",
    "GreedyEmbeddingHelper：适用于测试中采用Greedy策略sample的helper。\n",
    "CustomHelper：用户自定义的helper。\n",
    "\n",
    "4. beam search decoder\n",
    "除了上面提到的argmax算法和伯努利抽样算法以外，我们还可以使用Beam Search的抽样方法来获得最终的解码序列，\n",
    "在beam_search_decoder.py文件中，BeamSearchDecoder类是继承自Decoder类的，与之前的BasicDecoder类不同的是，\n",
    "BasicDecoder类需要设定helper参数，而这里的BeamSearchDecoder没有helper参数，因为它所采用的算法是Beam Search，\n",
    "其细节在该文件中有实现。\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers import core\n",
    "# 基础的seq2seq model\n",
    "class BaseModel(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams,#超参数\n",
    "        mode,#train/eval/infer/\n",
    "        iterator,#迭代次数\n",
    "        source_vocab_table,#\n",
    "        target_vocab_table,#\n",
    "        reverse_target_vocab_table=None,#Lookup table mapping ids to target words. Only required in INFER mode. Defaults to None.\n",
    "        scope=None,#模型的scope\n",
    "        extra_args=None):#model_helper.ExtraArgs, for passing customizable functions.\n",
    "        self.iterator = iterator\n",
    "        self.mode = mode\n",
    "        self.src_vocab_table = source_vocab_table\n",
    "        self.tgt_vocab_table = target_vocab_table\n",
    "\n",
    "        self.src_vocab_size = hparams.src_vocab_size\n",
    "        self.tgt_vocab_size = hparams.tgt_vocab_size\n",
    "        self.num_gpus = hparams.num_gpus\n",
    "        self.time_major = hparams.time_major\n",
    "\n",
    "\n",
    "        self.single_cell_fn = None\n",
    "        # extra_args: to make it flexible for adding external customizable code\n",
    "        if extra_args:\n",
    "            self.single_cell_fn = extra_args.single_cell_fn\n",
    "        # Set num layers\n",
    "        self.num_encoder_layers = hparams.num_encoder_layers\n",
    "        self.num_decoder_layers = hparams.num_decoder_layers\n",
    "        # Set num residual layers\n",
    "        if hasattr(hparams, \"num_residual_layers\"):  # compatible common_test_utils\n",
    "            self.num_encoder_residual_layers = hparams.num_residual_layers\n",
    "            self.num_decoder_residual_layers = hparams.num_residual_layers\n",
    "        else:\n",
    "            self.num_encoder_residual_layers = hparams.num_encoder_residual_layers\n",
    "            self.num_decoder_residual_layers = hparams.num_decoder_residual_layers\n",
    "\n",
    "        # Initializer\n",
    "        initializer=get_initializer(hparams.init_op, hparams.random_seed, hparams.init_weight)\n",
    "        tf.get_variable_scope().set_initializer(initializer)\n",
    "\n",
    "        # Projection\n",
    "        with tf.variable_scope(scope or 'build_network'):\n",
    "            with tf.variable_scope(\"decoder/output_projection\"):\n",
    "                self.output_layer = core.Dense(hparams.tgt_vocab_size, use_bias=False, name=\"output_projection\")\n",
    "\n",
    "        ## Train graph\n",
    "        res = self.build_graph(hparams, scope=scope)\n",
    "\n",
    "        if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            self.train_loss = res[1]\n",
    "            self.word_count=tf.reduce_sum(self.iterator.source_sequence_length)+tf.reduce_sum(self.iterator.target_sequence_length)\n",
    "        elif self.mode == tf.contrib.learn.ModeKeys.EVAL:\n",
    "            self.eval_loss = res[1]\n",
    "        elif self.mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            self.infer_logits, _, self.final_context_state, self.sample_id = res\n",
    "            self.sample_words = reverse_target_vocab_table.lookup(tf.to_int64(self.sample_id))\n",
    "\n",
    "        if self.mode != tf.contrib.learn.ModeKeys.INFER:\n",
    "            self.predict_count = tf.reduce_sum(self.iterator.target_sequence_length)\n",
    "\n",
    "        self.global_step= tf.Variable(0,trainable=False)\n",
    "        params = tf.trainable_variables()\n",
    "\n",
    "        if self.mode == tf.contrib.learn.ModeKeys.TRAIN:\n",
    "            self.learning_rate = tf.constant(hparams.learning_rate)\n",
    "            # warm-up\n",
    "            self.learning_rate = self._get_learning_rate_warmup(hparams)\n",
    "            # decay\n",
    "            self.learning_rate = self._get_learning_rate_decay(hparams)\n",
    "        \n",
    "        \n",
    "    \"\"\"Subclass must implement this method.\"\"\"\n",
    "    def build_graph(self, hparams, scope=None):\n",
    "        with tf.variable_scope(scope or \"dynamic_seq2seq\", dtype=dtype):\n",
    "            # Encoder\n",
    "            encoder_outputs, encoder_state = self._build_encoder(hparams)\n",
    "            ## Decoder\n",
    "            logits, sample_id, final_context_state = self._build_decoder(encoder_outputs, encoder_state, hparams)\n",
    "            ## Loss\n",
    "            if self.mode != tf.contrib.learn.ModeKeys.INFER:\n",
    "                with tf.device(get_device_str(self.num_encoder_layers - 1,self.num_gpus)):\n",
    "                    loss = self._compute_loss(logits)\n",
    "            else:\n",
    "                loss = None\n",
    "            return logits, loss, final_context_state, sample_id\n",
    "    \n",
    "    def _compute_loss(self,logits):\n",
    "        target_output = self.iterator.target_output\n",
    "        if self.time_major:\n",
    "            target_output = tf.transpose(target_output)\n",
    "        max_time = self.get_max_time(target_output)\n",
    "        #sparse_softmax_cross_entropy_with_logits:lables接受直接的数字标签\n",
    "        crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target_output,logits=logits)\n",
    "        '''\n",
    "        tf.sequence_mask([1, 3, 2], 5)  # [[True, False, False, False, False],\n",
    "                                  #  [True, True, True, False, False],\n",
    "                                  #  [True, True, False, False, False]]\n",
    "\n",
    "        tf.sequence_mask([[1, 3],[2,0]])  # [[[True, False, False],\n",
    "                                    #   [True, True, True]],\n",
    "                                    #  [[True, True, False],\n",
    "                                    #   [False, False, False]]]\n",
    "        '''\n",
    "        target_weights = tf.sequence_mask(self.iterator.target_sequence_length,max_time,dtype=logits.dtype)\n",
    "        if self.time_major:\n",
    "            target_weights=tf.transpose(target_weights)\n",
    "        \n",
    "        loss = tf.reduce_sum(crossent*target_weights)/tf.to_float(self.batch_size)\n",
    "        return loss\n",
    "    \n",
    "    def _get_learning_rate_warmup(self,hparams):\n",
    "        \"\"\"Get learning rate warmup.调整学习率退火方案和动量参数的SGD不仅可以与Adam竞争，而且收敛速度更快。\"\"\"\n",
    "        warmup_steps = hparams.warmup_steps\n",
    "        warmup_scheme = hparams.warmup_scheme\n",
    "        # When step < warmup_steps,\n",
    "        #   learing_rate *= warmup_factor ** (warmup_steps - step)\n",
    "        if warmup_scheme == \"t2t\":\n",
    "            # 0.01^(1/warmup_steps): we start with a lr, 100 times smaller\n",
    "            warmup_factor = tf.exp(tf.log(0.01) / warmup_steps)\n",
    "            inv_decay = warmup_factor**(tf.to_float(warmup_steps - self.global_step))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown warmup scheme %s\" % warmup_scheme)\n",
    "        return tf.cond(self.global_step < hparams.warmup_steps,\n",
    "                       lambda: inv_decay * self.learning_rate,\n",
    "                       lambda: self.learning_rate,\n",
    "                       name=\"learning_rate_warump_cond\")\n",
    "    \n",
    "    def _get_learning_rate_decay(self, hparams):\n",
    "        \"\"\"Get learning rate decay.\"\"\"\n",
    "        if hparams.decay_scheme in [\"luong5\", \"luong10\", \"luong234\"]:\n",
    "            decay_factor = 0.5\n",
    "            if hparams.decay_scheme == \"luong5\":\n",
    "                start_decay_step = int(hparams.num_train_steps / 2)\n",
    "                decay_times = 5\n",
    "            elif hparams.decay_scheme == \"luong10\":\n",
    "                start_decay_step = int(hparams.num_train_steps / 2)\n",
    "                decay_times = 10\n",
    "            elif hparams.decay_scheme == \"luong234\":\n",
    "                start_decay_step = int(hparams.num_train_steps * 2 / 3)\n",
    "                decay_times = 4\n",
    "            remain_steps = hparams.num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / decay_times)\n",
    "        elif not hparams.decay_scheme:  # no decay\n",
    "            start_decay_step = hparams.num_train_steps\n",
    "            decay_steps = 0\n",
    "            decay_factor = 1.0\n",
    "        elif hparams.decay_scheme:\n",
    "            raise ValueError(\"Unknown decay scheme %s\" % hparams.decay_scheme)\n",
    "        return tf.cond(self.global_step < start_decay_step,\n",
    "                        lambda: self.learning_rate,\n",
    "                        lambda: tf.train.exponential_decay(\n",
    "                        self.learning_rate,\n",
    "                        (self.global_step - start_decay_step),\n",
    "                        decay_steps, decay_factor, staircase=True),\n",
    "                        name=\"learning_rate_decay_cond\")\n",
    "    def init_embeddings(self, hparams, scope):\n",
    "        \"\"\"Init embeddings.\"\"\"\n",
    "        self.embedding_encoder, self.embedding_decoder = (\n",
    "            model_helper.create_emb_for_encoder_and_decoder(\n",
    "                share_vocab=hparams.share_vocab,\n",
    "                src_vocab_size=self.src_vocab_size,\n",
    "                tgt_vocab_size=self.tgt_vocab_size,\n",
    "                src_embed_size=hparams.num_units,\n",
    "                tgt_embed_size=hparams.num_units,\n",
    "                num_partitions=hparams.num_embeddings_partitions,\n",
    "                src_vocab_file=hparams.src_vocab_file,\n",
    "                tgt_vocab_file=hparams.tgt_vocab_file,\n",
    "                src_embed_file=hparams.src_embed_file,\n",
    "                tgt_embed_file=hparams.tgt_embed_file,\n",
    "                scope=scope,))\n",
    "    def train(self, sess):\n",
    "        assert self.mode == tf.contrib.learn.ModeKeys.TRAIN\n",
    "        return sess.run([self.update,\n",
    "                         self.train_loss,\n",
    "                         self.predict_count,\n",
    "                         self.train_summary,\n",
    "                         self.global_step,\n",
    "                         self.word_count,\n",
    "                         self.batch_size,\n",
    "                         self.grad_norm,\n",
    "                         self.learning_rate])\n",
    "    def eval(self, sess):\n",
    "        assert self.mode == tf.contrib.learn.ModeKeys.EVAL\n",
    "        return sess.run([self.eval_loss,\n",
    "                         self.predict_count,\n",
    "                         self.batch_size])\n",
    "    def _get_infer_maximum_iterations(self, hparams, source_sequence_length):\n",
    "        \"\"\"Maximum decoding steps at inference time.\"\"\"\n",
    "        if hparams.tgt_max_len_infer:\n",
    "            maximum_iterations=hparams.tgt_max_len_infer\n",
    "        else:\n",
    "            # TODO(thangluong): add decoding_length_factor flag\n",
    "            decoding_length_factor = 2.0\n",
    "            max_encoder_length = tf.reduce_max(source_sequence_length)\n",
    "            maximum_iterations = tf.to_int32(tf.round(tf.to_float(max_encoder_length)*decoding_length_factor))#干嘛用的？\n",
    "        return maximum_iterations\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def _build_encoder(self,hparams):\n",
    "        pass\n",
    "    def _build_encoder_cell(self, hparams, num_layers, num_residual_layers,base_gpu=0):\n",
    "        return create_rnn_cell(\n",
    "            unit_type=hparams.unit_type,\n",
    "            num_units=hparams.num_units,\n",
    "            num_layers=num_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            forget_bias=hparams.forget_bias,\n",
    "            dropout=hparams.dropout,\n",
    "            num_gpus=hparams.num_gpus,\n",
    "            mode=self.mode,\n",
    "            base_gpu=base_gpu,\n",
    "            single_cell_fn=self.single_cell_fn)\n",
    "    @abstractmethod\n",
    "    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,source_sequence_length):\n",
    "        pass\n",
    "    def _build_decoder(self,encoder_outputs,encoder_state,hparams):\n",
    "        \"\"\"Build and run a RNN decoder with a final projection layer.\"\"\"\n",
    "        tgt_sos_id=tf.cast(self.tgt_vocab_table.lookup(tf.constant(hparams.sos)),tf.int32)#何用？\n",
    "        tgt_eos_id=tf.cast(self.tgt_vocab_table.lookup(tf.constant(hparams.eos)),tf.int32)\n",
    "        iterator=self.iterator\n",
    "        \n",
    "        # maximum_iteration: The maximum decoding steps.\n",
    "        maximum_iterations = self._get_infer_maximum_iterations(hparams,iterator.source_sequence_length)\n",
    "        \n",
    "        ## Decoder.\n",
    "        with tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            cell, decoder_initial_state= self._build_decoder_cell(hparams, encoder_outputs, encoder_state,iterator.source_sequence_length)\n",
    "            \n",
    "            ## Train or eval\n",
    "            if self.mode != tf.contrib.learn.ModeKeys.INFER:\n",
    "                # decoder_emp_inp: [max_time, batch_size, num_units]\n",
    "                target_input = iterator.target_input\n",
    "                if self.time_major:\n",
    "                    target_input = tf.transpose(target_input)\n",
    "                decoder_emb_inp = tf.nn.embedding_lookup(self.embedding_decoder,target_input)\n",
    "                \n",
    "                # Helper\n",
    "                #A helper for use during training. Only reads inputs.Returned sample_ids are the argmax of the RNN output logits.\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(\n",
    "                    decoder_emb_inp, iterator.target_sequence_length,\n",
    "                    time_major=self.time_major)\n",
    "                \n",
    "                #Decoder\n",
    "                my_decoder = tf.contrib.seq2seq.BasicDecoder(cell,helper,decoder_initial_state,)\n",
    "                \n",
    "                # Dynamic decoding\n",
    "                outputs , final_context_state,_=tf.contrib.seq2seq.dynamic_decode(my_decoder,\n",
    "                                                                                  output_time_major=self.time_major,\n",
    "                                                                                  swap_memory=True,\n",
    "                                                                                  scope=decoder_scope)\n",
    "                sample_id = outputs.sample_id\n",
    "                #啥意思？\n",
    "                # Note: there's a subtle difference here between train and inference.\n",
    "                # We could have set output_layer when create my_decoder\n",
    "                #   and shared more code between train and inference.\n",
    "                # We chose to apply the output_layer to all timesteps for speed:\n",
    "                #   10% improvements for small models & 20% for larger ones.\n",
    "                # If memory is a concern, we should apply output_layer per timestep.\n",
    "                logits = self.output_layer(outputs.rnn_output)\n",
    "            ## Inference\n",
    "            else:\n",
    "                beam_width = hparams.beam_width\n",
    "                length_penalty_weight = hparams.length_penalty_weight\n",
    "                start_tokens = tf.fill([self.batch_size], tgt_sos_id)\n",
    "                end_token = tgt_eos_id\n",
    "                \n",
    "                if beam_width > 0:\n",
    "                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                        cell=cell,\n",
    "                        embedding=self.embedding_decoder,\n",
    "                        start_tokens=start_tokens,\n",
    "                        end_token=end_token,\n",
    "                        initial_state=decoder_initial_state,\n",
    "                        beam_width=beam_width,\n",
    "                        output_layer=self.output_layer,\n",
    "                        length_penalty_weight=length_penalty_weight)\n",
    "                else:\n",
    "                    # Helper,抽样方法，不如直接beamsearch\n",
    "                    sampling_temperature = hparams.sampling_temperature\n",
    "                    if sampling_temperature > 0.0:\n",
    "                        helper=tf.contrib.seq2seq.SampleEmbeddingHelper(\n",
    "                            self.embedding_decoder, start_tokens, end_token,\n",
    "                            softmax_temperature=sampling_temperature,\n",
    "                            seed=hparams.random_seed)\n",
    "                    else:\n",
    "                        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embedding_decoder, start_tokens, end_token)\n",
    "                    \n",
    "                    # Decoder\n",
    "                    my_decoder = tf.contrib.seq2seq.BasicDecoder(\n",
    "                        cell=cell,\n",
    "                        helper=helper,\n",
    "                        decoder_initial_state=decoder_initial_state,\n",
    "                        output_layer=self.output_layer)\n",
    "                    \n",
    "                # Dynamic decoding\n",
    "                outputs,final_context_state,_ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    my_decoder,\n",
    "                    maximum_iterations=maximum_iterations,\n",
    "                    output_time_major=self.time_major,\n",
    "                    swap_memory=True,\n",
    "                    scope=decoder_scope)\n",
    "                \n",
    "                if beam_width>0:\n",
    "                    logits = tf.no_op()\n",
    "                    sample_id = outputs.predicted_ids\n",
    "                else:\n",
    "                    logits = outputs.rnn_output\n",
    "                    sample_id = outputs.sample_id\n",
    "        return logits, sample_id, final_context_state\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态rnn注意力seq2seq\n",
    "class Model(BaseModel):\n",
    "    def _build_bidirectional_rnn(self, inputs, sequence_length,\n",
    "                               dtype, hparams,\n",
    "                               num_bi_layers,\n",
    "                               num_bi_residual_layers,\n",
    "                               base_gpu=0):\n",
    "        \"\"\"Create and call biddirectional RNN cells.\"\"\"\n",
    "        fw_cell=self._build_encoder_cell(hparams,num_bi_layers,num_bi_residual_layers,base_gpu=base_gpu)\n",
    "        bw_cell=self._build_encoder_cell(hparams,num_bi_layers,num_bi_residual_layers,base_gpu=(base_gpu+num_bi_residual_layers))\n",
    "        bi_outputs, bi_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            fw_cell,\n",
    "            bw_cell,\n",
    "            inputs,\n",
    "            dtype=dtype,\n",
    "            sequence_length=sequence_length,\n",
    "            time_major=self.time_major,\n",
    "            swap_memory=True)\n",
    "        return tf.concat(bi_outputs,-1),bi_state\n",
    "        \n",
    "        \n",
    "    def _build_encoder(self,hparams):\n",
    "        num_layers=self.num_encoder_layers\n",
    "        num_residual_layers = self.num_encoder_residual_layers\n",
    "        iterator = self.iterator\n",
    "        \n",
    "        source = iterator.source\n",
    "        if self.time_major:\n",
    "            source = tf.transpose(source)\n",
    "        with tf.variable_scope(\"encoder\") as scope:\n",
    "            dtype = scope.dtype\n",
    "            # Look up embedding, emp_inp: [max_time, batch_size, num_units]\n",
    "            encoder_emb_inp = tf.nn.embedding_lookup(self.embedding_encoder,source)\n",
    "            \n",
    "            # Encoder_outputs: [max_time, batch_size, num_units]\n",
    "            if hparams.encoder_type == \"uni\":\n",
    "                cell = self._build_encoder_cell(hparams, num_layers, num_residual_layers)\n",
    "                \n",
    "                encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "                    cell,\n",
    "                    encoder_emb_inp,\n",
    "                    dtype=dtype,\n",
    "                    sequence_length=iterator.source_sequence_length,\n",
    "                    time_major=self.time_major,\n",
    "                    swap_memory=True)\n",
    "            elif hparams.encoder_type == \"bi\":\n",
    "                num_bi_layers = int(num_layers / 2)\n",
    "                num_bi_residual_layers = int(num_residual_layers / 2)\n",
    "                encoder_outputs,bi_encoder_state=self._build_bidirectional_rnn(\n",
    "                    inputs=encoder_emb_inp,\n",
    "                    sequence_length=iterator.source_sequence_length,\n",
    "                    dtype=dtype,\n",
    "                    hparams=hparams,\n",
    "                    num_bi_layers=num_bi_layers,\n",
    "                    num_bi_residual_layers=num_bi_residual_layers)\n",
    "                if num_bi_layers == 1:\n",
    "                    encoder_state = bi_encoder_state\n",
    "                else:\n",
    "                    # alternatively concat forward and backward states\n",
    "                    encoder_state = []\n",
    "                    for layer_id in range(num_bi_layers):\n",
    "                        encoder_state.append(bi_encoder_state[0][layer_id])  # forward\n",
    "                        encoder_state.append(bi_encoder_state[1][layer_id])  # backward\n",
    "                    encoder_state = tuple(encoder_state)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown encoder_type %s\" % hparams.encoder_type)\n",
    "        return encoder_outputs, encoder_state\n",
    "    \n",
    "    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,source_sequence_length):\n",
    "        if hparams.attention:\n",
    "            raise ValueError(\"BasicModel doesn't support attention.\")\n",
    "        \n",
    "        cell = create_rnn_cell(\n",
    "            unit_type=hparams.unit_type,\n",
    "            num_units=hparams.num_units,\n",
    "            num_layers=self.num_decoder_layers,\n",
    "            num_residual_layers=self.num_decoder_residual_layers,\n",
    "            forget_bias=hparams.forget_bias,\n",
    "            dropout=hparams.dropout,\n",
    "            num_gpus=self.num_gpus,\n",
    "            mode=self.mode,\n",
    "            single_cell_fn=self.single_cell_fn)\n",
    "        # For beam search, we need to replicate encoder infos beam_width times\n",
    "        #使用tile_batch函数处理一下，将batch_size扩展beam_size倍变成batch_size*beam_size\n",
    "        \"\"\"\n",
    "        使用tile_batch函数处理一下，将batch_size扩展beam_size倍变成batch_size*beam_size\n",
    "        beam search只在test的时候需要。训练的时候知道正确答案，并不需要再进行这个搜索。\n",
    "        test的时候，假设词表大小为3，内容为a，b，c。beam size是2\n",
    "        decoder解码的时候：\n",
    "        1： 生成第1个词的时候，选择概率最大的2个词，假设为a,c,那么当前序列就是a,c\n",
    "        2：生成第2个词的时候，我们将当前序列a和c，分别与词表中的所有词进行组合，得到新的6个序\n",
    "        列aa ab ac ca cb cc,然后从其中选择2个得分最高的，作为当前序列，假如为aa cb\n",
    "        3：后面会不断重复这个过程，直到遇到结束符为止。最终输出2个得分最高的序列。\n",
    "        \"\"\"\n",
    "        if self.mode == tf.contrib.learn.ModeKeys.INFER and hparams.beam_width > 0:\n",
    "            decoder_initial_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=hparams.beam_width)\n",
    "        else:\n",
    "            decoder_initial_state = encoder_state\n",
    "        return cell, decoder_initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Attention-based sequence-to-sequence model with dynamic RNN support.\"\"\"\n",
    "class AttentionModel(Model):\n",
    "    def __init__(self,\n",
    "           hparams,\n",
    "           mode,\n",
    "           iterator,\n",
    "           source_vocab_table,\n",
    "           target_vocab_table,\n",
    "           reverse_target_vocab_table=None,\n",
    "           scope=None,\n",
    "           extra_args=None):\n",
    "        # Set attention_mechanism_fn\n",
    "        if extra_args and extra_args.attention_mechanism_fn:\n",
    "            self.attention_mechanism_fn = extra_args.attention_mechanism_fn\n",
    "        else:\n",
    "            self.attention_mechanism_fn = create_attention_mechanism\n",
    "\n",
    "        super(AttentionModel, self).__init__(\n",
    "            hparams=hparams,\n",
    "            mode=mode,\n",
    "            iterator=iterator,\n",
    "            source_vocab_table=source_vocab_table,\n",
    "            target_vocab_table=target_vocab_table,\n",
    "            reverse_target_vocab_table=reverse_target_vocab_table,\n",
    "            scope=scope,\n",
    "            extra_args=extra_args)\n",
    "        if self.mode == tf.contrib.learn.ModeKeys.INFER:\n",
    "            self.infer_summary = self._get_infer_summary(hparams)\n",
    "    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,source_sequence_length): \n",
    "        \"\"\"Build a RNN cell with attention mechanism that can be used by decoder.\"\"\"\n",
    "        attention_option = hparams.attention\n",
    "        attention_architecture = hparams.attention_architecture\n",
    "        if attention_architecture != \"standard\":\n",
    "            raise ValueError(\"Unknown attention architecture %s\" % attention_architecture)\n",
    "        num_units = hparams.num_units\n",
    "        num_layers = self.num_decoder_layers\n",
    "        num_residual_layers = self.num_decoder_residual_layers\n",
    "        beam_width = hparams.beam_width\n",
    "        dtype = tf.float32\n",
    "        if self.time_major:\n",
    "            memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "        else:\n",
    "            memory = encoder_outputs\n",
    "        \n",
    "        if self.mode == tf.contrib.learn.ModeKeys.INFER and beam_width > 0:\n",
    "            memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n",
    "            source_sequence_length = tf.contrib.seq2seq.tile_batch(source_sequence_length, multiplier=beam_width)\n",
    "            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "            batch_size = self.batch_size * beam_width\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        attention_mechanism = self.attention_mechanism_fn(attention_option, num_units, memory, source_sequence_length, self.mode)\n",
    "        cell=create_rnn_cell(\n",
    "            unit_type=hparams.unit_type,\n",
    "            num_units=num_units,\n",
    "            num_layers=num_layers,\n",
    "            num_residual_layers=num_residual_layers,\n",
    "            forget_bias=hparams.forget_bias,\n",
    "            dropout=hparams.dropout,\n",
    "            num_gpus=self.num_gpus,\n",
    "            mode=self.mode,\n",
    "            single_cell_fn=self.single_cell_fn)\n",
    "            # Only generate alignment in greedy INFER mode.\n",
    "        alignment_history = (self.mode == tf.contrib.learn.ModeKeys.INFER and\n",
    "                             beam_width == 0)\n",
    "        cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "            cell,\n",
    "            attention_mechanism,\n",
    "            attention_layer_size=num_units,\n",
    "            alignment_history=alignment_history,\n",
    "            output_attention=hparams.output_attention,\n",
    "            name=\"attention\")\n",
    "        # TODO(thangluong): do we need num_layers, num_gpus?\n",
    "        cell = tf.contrib.rnn.DeviceWrapper(cell,model_helper.get_device_str(num_layers - 1, self.num_gpus))\n",
    "        \n",
    "        if hparams.pass_hidden_state:\n",
    "            decoder_initial_state = cell.zero_state(batch_size, dtype).clone(cell_state=encoder_state)\n",
    "        else:\n",
    "            decoder_initial_state = cell.zero_state(batch_size, dtype)\n",
    "        return cell, decoder_initial_state\n",
    "    \n",
    "    def _get_infer_summary(self, hparams):\n",
    "        if hparams.beam_width > 0:\n",
    "            return tf.no_op()\n",
    "        return _create_attention_images_summary(self.final_context_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "    \"\"\"A MultiCell with GNMT attention style.\"\"\"\n",
    "    def __init__(self, attention_cell, cells, use_new_attention=False):\n",
    "        \"\"\"Creates a GNMTAttentionMultiCell.\n",
    "        Args:\n",
    "          attention_cell: An instance of AttentionWrapper.\n",
    "          cells: A list of RNNCell wrapped with AttentionInputWrapper.\n",
    "          use_new_attention: Whether to use the attention generated from current\n",
    "            step bottom layer's output. Default is False.\n",
    "        \"\"\"\n",
    "        cells = [attention_cell] + cells\n",
    "        self.use_new_attention = use_new_attention\n",
    "        super(GNMTAttentionMultiCell,self).__init__(cells,state_is_tuple=True)\n",
    "        \n",
    "    def __call__(self,inputs,state,scope=None):\n",
    "        \"\"\"Run the cell with bottom layer's attention copied to all upper layers.\"\"\"\n",
    "        if not nest.is_sequence(state): \n",
    "            raise ValueError(\"Expected state to be a tuple of length %d, but received: %s\"% (len(self.state_size), state))\n",
    "        with tf.variable_scope(scope or \"multi_rnn_cell\"):\n",
    "            new_states = []\n",
    "            with tf.variable_scope(\"cell_0_attention\"):\n",
    "                attention_cell = self._cells[0]\n",
    "                attention_state = state[0]\n",
    "                cur_inp, new_attention_state = attention_cell(inputs, attention_state)\n",
    "                new_states.append(new_attention_state)\n",
    "            \n",
    "            for i in range(1,len(self._cells)):\n",
    "                with tf.variable_scope(\"cell_%d\" % i):\n",
    "                    cell = self.cells[i]\n",
    "                    cur_state = state[i]\n",
    "                    \n",
    "                    if self.use_new_attention:\n",
    "                        cur_inp = tf.concat([cur_inp,new_attention_state.attention],-1)\n",
    "                    else:\n",
    "                        cur_inp = tf.concat([cur_inp,attention_state.attention],-1)\n",
    "                    \n",
    "                    cur_inp,new_state =cell(cur_inp,cur_state)\n",
    "                    new_states.append(new_state)\n",
    "        return cur_inp,tuple(new_states)\n",
    "    \n",
    "    def gnmt_residual_fn(inputs,outputs):\n",
    "        \"\"\"Residual function that handles different inputs and outputs inner dims.\n",
    "        # \n",
    "        Args:\n",
    "        inputs: cell inputs, this is actual inputs concatenated with the attention\n",
    "          vector.\n",
    "        outputs: cell outputs\n",
    "\n",
    "        Returns:\n",
    "        outputs + actual inputs\n",
    "        \"\"\"\n",
    "        \n",
    "        def split_input(inp,out):\n",
    "            out_dim = out.get_shape().as_list()[-1]\n",
    "            inp_dim = inp.get_shape().as_list()[-1]\n",
    "            eturn tf.split(inp, [out_dim, inp_dim - out_dim], axis=-1)\n",
    "        actual_inputs, _ = nest.map_structure(split_input, inputs, outputs)\n",
    "        def assert_shape_match(inp, out):\n",
    "            inp.get_shape().assert_is_compatible_with(out.get_shape())\n",
    "        nest.assert_same_structure(actual_inputs, outputs)\n",
    "        nest.map_structure(assert_shape_match, actual_inputs, outputs)\n",
    "        return nest.map_structure(lambda inp, out: inp + out, actual_inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GNMT attention sequence-to-sequence model with dynamic RNN support.\"\"\"\n",
    "class GNMTModel(AttentionModel):\n",
    "    def __init__(self,\n",
    "                hparams,\n",
    "                mode,\n",
    "                iterator,\n",
    "                source_vocab_table,\n",
    "                target_vocab_table,\n",
    "                reverse_target_vocab_table=None,\n",
    "                scope=None,\n",
    "                extra_args=None):\n",
    "        super(GNMTModel, self).__init__(\n",
    "            hparams=hparams,\n",
    "            mode=mode,\n",
    "            iterator=iterator,\n",
    "            source_vocab_table=source_vocab_table,\n",
    "            target_vocab_table=target_vocab_table,\n",
    "            reverse_target_vocab_table=reverse_target_vocab_table,\n",
    "            scope=scope,\n",
    "            extra_args=extra_args)\n",
    "    def _build_encoder(self, hparams):\n",
    "        \"\"\"Build a GNMT encoder.\"\"\"\n",
    "        if hparams.encoder_type == \"uni\" or hparams.encoder_type == \"bi\":\n",
    "            return super(GNMTModel, self)._build_encoder(hparams)if hparams\n",
    "        if hparams.encoder_type != \"gnmt\":\n",
    "            raise ValueError(\"Unknown encoder_type %s\" % hparams.encoder_type)\n",
    "        \n",
    "        # Build GNMT encoder.\n",
    "        num_bi_layers = 1\n",
    "        num_uni_layers = self.num_encoder_layers - num_bi_layers\n",
    "        iterator = self.iterator\n",
    "        source = iterator.source\n",
    "        if self.time_major:\n",
    "            source = tf.transpose(source)\n",
    "        with tf.variable_scope(\"encoder\") as scope:\n",
    "            dtype = scope.dtype\n",
    "            # Look up embedding, emp_inp: [max_time, batch_size, num_units]\n",
    "            #   when time_major = True\n",
    "            #一层bi-rnn加多层uni-rnn\n",
    "            encoder_emb_inp = tf.nn.embedding_lookup(self.embedding_encoder,\n",
    "                                       source)\n",
    "            \n",
    "            # Execute _build_bidirectional_rnn from Model class\n",
    "            bi_encoder_outputs, bi_encoder_state = self._build_bidirectional_rnn(\n",
    "                inputs=encoder_emb_inp,\n",
    "                sequence_length=iterator.source_sequence_length,\n",
    "                dtype=dtype,\n",
    "                hparams=hparams,\n",
    "                num_bi_layers=num_bi_layers,\n",
    "                num_bi_residual_layers=0,  # no residual connection\n",
    "            )\n",
    "            \n",
    "            uni_cell = create_rnn_cell(\n",
    "                unit_type=hparams.unit_type,\n",
    "                num_units=hparams.num_units,\n",
    "                num_layers=num_uni_layers,\n",
    "                num_residual_layers=self.num_encoder_residual_layers,\n",
    "                forget_bias=hparams.forget_bias,\n",
    "                dropout=hparams.dropout,\n",
    "                num_gpus=self.num_gpus,\n",
    "                base_gpu=1,\n",
    "                mode=self.mode,\n",
    "                single_cell_fn=self.single_cell_fn)\n",
    "            # encoder_outputs: size [max_time, batch_size, num_units]\n",
    "            #   when time_major = True\n",
    "            encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n",
    "                uni_cell,\n",
    "                bi_encoder_outputs,\n",
    "                dtype=dtype,\n",
    "                sequence_length=iterator.source_sequence_length,\n",
    "                time_major=self.time_major)\n",
    "            \n",
    "            # Pass all encoder state except the first bi-directional layer's state to decoder.\n",
    "            encoder_state = (bi_encoder_state[1],) + ((encoder_state,) if num_uni_layers == 1 else encoder_state)\n",
    "        return encoder_outputs, encoder_state   \n",
    "    def _build_decoder_cell(self, hparams, encoder_outputs, encoder_state,source_sequence_length):\n",
    "        \"\"\"Build a RNN cell with GNMT attention architecture.\"\"\"\n",
    "        # Standard attention\n",
    "        if hparams.attention_architecture == \"standard\":\n",
    "            return super(GNMTModel, self)._build_decoder_cell(\n",
    "                hparams, encoder_outputs, encoder_state, source_sequence_length)\n",
    "        \n",
    "        # GNMT attention\n",
    "        attention_option = hparams.attention\n",
    "        attention_architecture = hparams.attention_architecture\n",
    "        num_units = hparams.num_units\n",
    "        beam_width = hparams.beam_width\n",
    "        \n",
    "        dtype = tf.float32\n",
    "        \n",
    "        if self.time_major:\n",
    "            memory = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "        else:\n",
    "            memory = encoder_outputs\n",
    "        \n",
    "        if self.mode == tf.contrib.learn.ModeKeys.INFER and beam_width > 0:\n",
    "            memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=beam_width)\n",
    "            source_sequence_length = tf.contrib.seq2seq.tile_batch(source_sequence_length, multiplier=beam_width)\n",
    "            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=beam_width)\n",
    "            batch_size = self.batch_size * beam_width\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        attention_mechanism = self.attention_mechanism_fn(attention_option, num_units, memory, source_sequence_length, self.mode)\n",
    "        cell_list = model_helper._cell_list(  # pylint: disable=protected-access\n",
    "            unit_type=hparams.unit_type,\n",
    "            num_units=num_units,\n",
    "            num_layers=self.num_decoder_layers,\n",
    "            num_residual_layers=self.num_decoder_residual_layers,\n",
    "            forget_bias=hparams.forget_bias,\n",
    "            dropout=hparams.dropout,\n",
    "            num_gpus=self.num_gpus,\n",
    "            mode=self.mode,\n",
    "            single_cell_fn=self.single_cell_fn,\n",
    "            residual_fn=gnmt_residual_fn)\n",
    "        # Only wrap the bottom layer with the attention mechanism.\n",
    "        attention_cell = cell_list.pop(0)\n",
    "        # Only generate alignment in greedy INFER mode.\n",
    "        alignment_history = (self.mode == tf.contrib.learn.ModeKeys.INFER and beam_width == 0)\n",
    "        attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                                    attention_cell,\n",
    "                                    attention_mechanism,\n",
    "                                    attention_layer_size=None,  # don't use attention layer.\n",
    "                                    output_attention=False,\n",
    "                                    alignment_history=alignment_history,\n",
    "                                    name=\"attention\")\n",
    "        if attention_architecture == \"gnmt\":\n",
    "            cell = GNMTAttentionMultiCell(attention_cell, cell_list)\n",
    "        elif attention_architecture == \"gnmt_v2\":\n",
    "            cell = GNMTAttentionMultiCell(attention_cell, cell_list, use_new_attention=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown attention_architecture %s\" % attention_architecture)\n",
    "        if hparams.pass_hidden_state:\n",
    "            decoder_initial_state = tuple(zs.clone(cell_state=es)\n",
    "            if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState) else es\n",
    "            for zs, es in zip(cell.zero_state(batch_size, dtype), encoder_state))\n",
    "        else:\n",
    "            decoder_initial_state = cell.zero_state(batch_size, dtype)\n",
    "        return cell,decoder_initial_state\n",
    "    \n",
    "    \n",
    "    def _get_infer_summary(self, hparams):\n",
    "        # Standard attention\n",
    "        if hparams.attention_architecture == \"standard\":\n",
    "            return super(GNMTModel, self)._get_infer_summary(hparams)\n",
    "\n",
    "        # GNMT attention\n",
    "        if hparams.beam_width > 0:\n",
    "            return tf.no_op()\n",
    "        return attention_model._create_attention_images_summary(self.final_context_state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
